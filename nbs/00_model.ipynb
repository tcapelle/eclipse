{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "> from the paper from [Paletta et al](https://arxiv.org/pdf/2104.12419v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to implement as close as possible the architecture from the paper `ECLIPSE : Envisioning Cloud Induced Perturbations in Solar Energy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](images/eclipse_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from eclipse_pytorch.imports import *\n",
    "from eclipse_pytorch.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spatial Downsampler\n",
    "> A resnet encoder to get image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use any spatial downsampler as you want, but the paper states a simple resnet arch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpatialDownsampler(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=1)\n",
    "        self.blocks = nn.Sequential(ResBlock(64, 64, kernel_size=3, stride=2), \n",
    "                                    ResBlock(64, 128, kernel_size=3, stride=2), \n",
    "                                    ResBlock(128,256, kernel_size=3, stride=2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.blocks(self.conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SpatialDownsampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 256, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [torch.rand(1, 3, 64, 64) for _ in range(4)]\n",
    "features = torch.stack([sd(image) for image in images], dim=1)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TemporalModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, receptive_field, input_shape, start_out_channels=64, extra_in_channels=0,\n",
    "            n_spatial_layers_between_temporal_layers=0, use_pyramid_pooling=True):\n",
    "        super().__init__()\n",
    "        self.receptive_field = receptive_field\n",
    "        n_temporal_layers = receptive_field - 1\n",
    "\n",
    "        h, w = input_shape\n",
    "        modules = []\n",
    "\n",
    "        block_in_channels = in_channels\n",
    "        block_out_channels = start_out_channels\n",
    "\n",
    "        for _ in range(n_temporal_layers):\n",
    "            if use_pyramid_pooling:\n",
    "                use_pyramid_pooling = True\n",
    "                pool_sizes = [(2, h, w)]\n",
    "            else:\n",
    "                use_pyramid_pooling = False\n",
    "                pool_sizes = None\n",
    "            temporal = TemporalBlock(\n",
    "                block_in_channels,\n",
    "                block_out_channels,\n",
    "                use_pyramid_pooling=use_pyramid_pooling,\n",
    "                pool_sizes=pool_sizes,\n",
    "            )\n",
    "            spatial = [\n",
    "                Bottleneck3D(block_out_channels, block_out_channels, kernel_size=(1, 3, 3))\n",
    "                for _ in range(n_spatial_layers_between_temporal_layers)\n",
    "            ]\n",
    "            temporal_spatial_layers = nn.Sequential(temporal, *spatial)\n",
    "            modules.extend(temporal_spatial_layers)\n",
    "\n",
    "            block_in_channels = block_out_channels\n",
    "            block_out_channels += extra_in_channels\n",
    "\n",
    "        self.out_channels = block_in_channels\n",
    "\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input tensor to (batch, C, time, H, W)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = self.model(x)\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        return x[:, (self.receptive_field - 1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TemporalModel(256, 3, (8,8), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 128, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_encoded = tm(features)\n",
    "temp_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Future State Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FuturePrediction(128, 128, n_gru_blocks=4, n_res_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = torch.rand(1, 128, 8, 8)\n",
    "x = torch.rand(1, 4, 128, 8, 8)\n",
    "fp(x, hidden).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A. Segmentation Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bottleneck(\n",
       "  (layers): Sequential(\n",
       "    (conv_down_project): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (abn_down_project): Sequential(\n",
       "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (abn): Sequential(\n",
       "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_up_project): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (abn_up_project): Sequential(\n",
       "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (upsample_skip_proj): Interpolate()\n",
       "    (conv_skip_proj): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn_skip_proj): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = Bottleneck(256, 128, upsample=True)\n",
    "bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,256,32,32)\n",
    "bn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, sizes=[128,128,64], n_out=3):\n",
    "        super().__init__()\n",
    "        zsizes = zip(sizes[:-1], sizes[1:])\n",
    "        self.convs = nn.Sequential(*[Bottleneck(si, sf, upsample=True) for si,sf in zsizes], \n",
    "                                   Bottleneck(sizes[-1], sizes[-1], upsample=True), \n",
    "                                   ConvBlock(sizes[-1], n_out, kernel_size=1, activation=None))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us = Upsampler()\n",
    "\n",
    "x = torch.rand(1,128,32,32)\n",
    "us(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B. Irradiance Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IrradianceModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(ConvBlock(128, 64), \n",
    "                                   ConvBlock(64, 64),\n",
    "                                   nn.AdaptiveMaxPool2d(1)\n",
    "                                  )\n",
    "        self.linear = nn.Sequential(nn.Flatten(), \n",
    "                                    nn.BatchNorm1d(64),\n",
    "                                    nn.Linear(64, 1)\n",
    "                                   )\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.convs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = IrradianceModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 128, 32, 32)\n",
    "im(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything Together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Eclipse(nn.Module):\n",
    "    \"\"\"Not very parametric\"\"\"\n",
    "    def __init__(self, n_in=3, n_out=4, horizon=5, img_size=(128, 128), debug=False):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        self.spatial_downsampler = SpatialDownsampler(n_in)\n",
    "        self.temporal_model = TemporalModel(256, 3, input_shape=(img_size[0]//8, img_size[1]//8), start_out_channels=128)\n",
    "        self.future_prediction = FuturePrediction(128, 128, n_gru_blocks=4, n_res_layers=4)\n",
    "        self.upsampler = Upsampler(n_out=n_out)\n",
    "        self.irradiance = IrradianceModule()\n",
    "    \n",
    "    def zero_hidden(self, x, horizon):\n",
    "        bs, ch, h, w = x.shape\n",
    "        return x.new_zeros(bs, horizon, ch, h, w)\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        x = torch.stack([self.spatial_downsampler(img) for img in imgs], dim=1)\n",
    "        \n",
    "        #encode temporal model\n",
    "        states = self.temporal_model(x)\n",
    "        if self.debug: print(f'{states.shape=}')\n",
    "        \n",
    "        #get hidden state\n",
    "        present_state = states[:, -1:]\n",
    "        if self.debug: print(f'{present_state.shape=}')\n",
    "        \n",
    "        \n",
    "        # Prepare future prediction input\n",
    "        hidden_state = present_state.squeeze()\n",
    "        if self.debug: print(f'{hidden_state.shape=}')\n",
    "        \n",
    "        future_prediction_input = self.zero_hidden(hidden_state, self.horizon)\n",
    "        \n",
    "        # Recursively predict future states\n",
    "        future_states = self.future_prediction(future_prediction_input, hidden_state)\n",
    "\n",
    "        # Concatenate present state\n",
    "        future_states = torch.cat([present_state, future_states], dim=1)\n",
    "        if self.debug: print(f'{future_states.shape=}')\n",
    "        \n",
    "        #decode outputs\n",
    "        masks, irradiances = [], []\n",
    "\n",
    "        for state in future_states.unbind(dim=1):\n",
    "            masks.append(self.upsampler(state))\n",
    "            irradiances.append(self.irradiance(state))\n",
    "        return {'masks': masks, 'irradiances': torch.cat(irradiances, dim=-1)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse = Eclipse(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.shape=torch.Size([2, 2, 128, 16, 16])\n",
      "present_state.shape=torch.Size([2, 1, 128, 16, 16])\n",
      "hidden_state.shape=torch.Size([2, 128, 16, 16])\n",
      "future_states.shape=torch.Size([2, 6, 128, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 128, 128]), torch.Size([2, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = eclipse([torch.rand(2, 3, 128, 128) for _ in range(4)])\n",
    "\n",
    "preds['masks'][0].shape, preds['irradiances'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
